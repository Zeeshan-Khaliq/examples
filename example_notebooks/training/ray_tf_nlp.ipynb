{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205f1e20",
   "metadata": {},
   "source": [
    "### (Advanced) PopulationBasedTraining with Ray tune, TensorFlow, mlflow and Tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a2db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3dcdcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-29 20:08:46.337740: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Permute,\n",
    "                                     Dropout)\n",
    "from tensorflow.keras.layers import add, dot, concatenate\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from filelock import FileLock\n",
    "import os\n",
    "import argparse\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from ray import tune\n",
    "import mlflow\n",
    "from ray.tune.integration.mlflow import MLflowLoggerCallback, mlflow_mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe027cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    \"\"\"Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize(\"Bob dropped the apple. Where is the apple?\")\n",
    "    [\"Bob\", \"dropped\", \"the\", \"apple\", \".\", \"Where\", \"is\", \"the\", \"apple\", \"?\"]\n",
    "    \"\"\"\n",
    "    return [x.strip() for x in re.split(r\"(\\W+)?\", sent) if x and x.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "660bcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(lines, only_supporting=False):\n",
    "    \"\"\"Parse stories provided in the bAbi tasks format\n",
    "\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode(\"utf-8\").strip()\n",
    "        nid, line = line.split(\" \", 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if \"\\t\" in line:\n",
    "            q, a, supporting = line.split(\"\\t\")\n",
    "            q = tokenize(q)\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append(\"\")\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df31a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    \"\"\"Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    \"\"\"\n",
    "\n",
    "    def flatten(data):\n",
    "        return sum(data, [])\n",
    "\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data\n",
    "            if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e886af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(word_idx, story_maxlen, query_maxlen, data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen), np.array(answers))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54df0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(finish_fast=False):\n",
    "    # Get the file\n",
    "    try:\n",
    "        path = get_file(\n",
    "            \"babi-tasks-v1-2.tar.gz\",\n",
    "#             origin=\"https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\")\n",
    "            origin = \"s3://d2v-tmp/demo/data/qa/babi_tasks_1-20_v1-2.tar.gz\")\n",
    "    except Exception:\n",
    "        print(\n",
    "            \"Error downloading dataset, please download it manually:\\n\"\n",
    "            \"$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2\"  # noqa: E501\n",
    "            \".tar.gz\\n\"\n",
    "            \"$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\"  # noqa: E501\n",
    "        )\n",
    "        raise\n",
    "\n",
    "    # Choose challenge\n",
    "    challenges = {\n",
    "        # QA1 with 10,000 samples\n",
    "        \"single_supporting_fact_10k\": \"tasks_1-20_v1-2/en-10k/qa1_\"\n",
    "        \"single-supporting-fact_{}.txt\",\n",
    "        # QA2 with 10,000 samples\n",
    "        \"two_supporting_facts_10k\": \"tasks_1-20_v1-2/en-10k/qa2_\"\n",
    "        \"two-supporting-facts_{}.txt\",\n",
    "    }\n",
    "    challenge_type = \"single_supporting_fact_10k\"\n",
    "    challenge = challenges[challenge_type]\n",
    "\n",
    "    with tarfile.open(path) as tar:\n",
    "        train_stories = get_stories(tar.extractfile(challenge.format(\"train\")))\n",
    "        test_stories = get_stories(tar.extractfile(challenge.format(\"test\")))\n",
    "    if finish_fast:\n",
    "        train_stories = train_stories[:64]\n",
    "        test_stories = test_stories[:64]\n",
    "    return train_stories, test_stories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8dc493a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When using mlflow_mixin with Ray Client, it is recommended to use a remote tracking server. If you are using a MLflow tracking server backed by the local filesystem, then it must be setup on the server side and not on the client side.\n",
      "When using mlflow_mixin with Ray Client, it is recommended to use a remote tracking server. If you are using a MLflow tracking server backed by the local filesystem, then it must be setup on the server side and not on the client side.\n",
      "When using mlflow_mixin with Ray Client, it is recommended to use a remote tracking server. If you are using a MLflow tracking server backed by the local filesystem, then it must be setup on the server side and not on the client side.\n"
     ]
    }
   ],
   "source": [
    "class MemNNModel(tune.Trainable):\n",
    "    @mlflow_mixin\n",
    "    def build_model(self):\n",
    "        \"\"\"Helper method for creating the model\"\"\"\n",
    "        vocab = set()\n",
    "        for story, q, answer in self.train_stories + self.test_stories:\n",
    "            vocab |= set(story + q + [answer])\n",
    "        vocab = sorted(vocab)\n",
    "\n",
    "        # Reserve 0 for masking via pad_sequences\n",
    "        vocab_size = len(vocab) + 1\n",
    "        story_maxlen = max(\n",
    "            len(x) for x, _, _ in self.train_stories + self.test_stories)\n",
    "        query_maxlen = max(\n",
    "            len(x) for _, x, _ in self.train_stories + self.test_stories)\n",
    "\n",
    "        word_idx = {c: i + 1 for i, c in enumerate(vocab)}\n",
    "        self.inputs_train, self.queries_train, self.answers_train = (\n",
    "            vectorize_stories(word_idx, story_maxlen, query_maxlen,\n",
    "                              self.train_stories))\n",
    "        self.inputs_test, self.queries_test, self.answers_test = (\n",
    "            vectorize_stories(word_idx, story_maxlen, query_maxlen,\n",
    "                              self.test_stories))\n",
    "\n",
    "        # placeholders\n",
    "        input_sequence = Input((story_maxlen, ))\n",
    "        question = Input((query_maxlen, ))\n",
    "\n",
    "        # encoders\n",
    "        # embed the input sequence into a sequence of vectors\n",
    "        input_encoder_m = Sequential()\n",
    "        input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "        input_encoder_m.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "        # embed the input into a sequence of vectors of size query_maxlen\n",
    "        input_encoder_c = Sequential()\n",
    "        input_encoder_c.add(\n",
    "            Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "        input_encoder_c.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "        # embed the question into a sequence of vectors\n",
    "        question_encoder = Sequential()\n",
    "        question_encoder.add(\n",
    "            Embedding(\n",
    "                input_dim=vocab_size, output_dim=64,\n",
    "                input_length=query_maxlen))\n",
    "        question_encoder.add(Dropout(self.config.get(\"dropout\", 0.3)))\n",
    "        # output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "        # encode input sequence and questions (which are indices)\n",
    "        # to sequences of dense vectors\n",
    "        input_encoded_m = input_encoder_m(input_sequence)\n",
    "        input_encoded_c = input_encoder_c(input_sequence)\n",
    "        question_encoded = question_encoder(question)\n",
    "\n",
    "        # compute a \"match\" between the first input vector sequence\n",
    "        # and the question vector sequence\n",
    "        # shape: `(samples, story_maxlen, query_maxlen)`\n",
    "        match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "        match = Activation(\"softmax\")(match)\n",
    "\n",
    "        # add the match matrix with the second input vector sequence\n",
    "        response = add(\n",
    "            [match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "        response = Permute(\n",
    "            (2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "        # concatenate the match matrix with the question vector sequence\n",
    "        answer = concatenate([response, question_encoded])\n",
    "\n",
    "        # the original paper uses a matrix multiplication.\n",
    "        # we choose to use a RNN instead.\n",
    "        answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "        # one regularization layer -- more would probably be needed.\n",
    "        answer = Dropout(self.config.get(\"dropout\", 0.3))(answer)\n",
    "        answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "        # we output a probability distribution over the vocabulary\n",
    "        answer = Activation(\"softmax\")(answer)\n",
    "\n",
    "        # build the final model\n",
    "        model = Model([input_sequence, question], answer)\n",
    "        return model\n",
    "    \n",
    "    @mlflow_mixin\n",
    "    def setup(self, config):\n",
    "        with FileLock(os.path.expanduser(\"~/.tune.lock\")):\n",
    "            self.train_stories, self.test_stories = read_data(\n",
    "                config[\"finish_fast\"])\n",
    "        model = self.build_model()\n",
    "        rmsprop = RMSprop(\n",
    "            lr=self.config.get(\"lr\", 1e-3), rho=self.config.get(\"rho\", 0.9))\n",
    "        model.compile(\n",
    "            optimizer=rmsprop,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "        self.model = model\n",
    "        \n",
    "    @mlflow_mixin\n",
    "    def step(self):\n",
    "        # train\n",
    "        mlflow.tensorflow.autolog()\n",
    "        self.model.fit(\n",
    "            [self.inputs_train, self.queries_train],\n",
    "            self.answers_train,\n",
    "            batch_size=self.config.get(\"batch_size\", 32),\n",
    "            epochs=self.config.get(\"epochs\", 1),\n",
    "            validation_data=([self.inputs_test, self.queries_test],\n",
    "                             self.answers_test),\n",
    "            verbose=0)\n",
    "        _, accuracy = self.model.evaluate(\n",
    "            [self.inputs_train, self.queries_train],\n",
    "            self.answers_train,\n",
    "            verbose=0)\n",
    "        return {\"mean_accuracy\": accuracy}\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_dir):\n",
    "        file_path = checkpoint_dir + \"/model\"\n",
    "        self.model.save(file_path)\n",
    "        return file_path\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        # See https://stackoverflow.com/a/42763323\n",
    "        del self.model\n",
    "        self.model = load_model(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b234ff",
   "metadata": {},
   "source": [
    "## initialize a Ray cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b0b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "## start the ray cluster\n",
    "from ray_common import initialize_ray_cluster, stop_ray_cluster\n",
    "\n",
    "num_workers = 2\n",
    "cpu_per_worker=\"4000m\"\n",
    "ram_per_worker=\"4.0Gi\"\n",
    "\n",
    "\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_per_worker, ram_per_worker)\n",
    "ray_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b6befa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "    perturbation_interval=2,\n",
    "    hyperparam_mutations={\n",
    "        \"dropout\": lambda: np.random.uniform(0, 1),\n",
    "        \"lr\": lambda: 10**np.random.randint(-10, 0),\n",
    "        \"rho\": lambda: np.random.uniform(0, 1)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b08d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(os.environ.get('DATABASE_URL_NO_PARAMS'))\n",
    "mlflow.set_experiment(\"pbt_babi_memnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43d0411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Memory usage on this node: 2.8/15.6 GiB\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.6 GiB heap, 0.0/2.55 GiB objects (0.0/1.0 example-resource-b, 0.0/1.0 example-resource-a)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result logdir: /root/ray_results/pbt_babi_memnn\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Number of trials: 2/2 (2 PENDING)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+----------+-------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | Trial name             | status   | loc   |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m |------------------------+----------+-------|\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00000 | PENDING  |       |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00001 | PENDING  |       |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+----------+-------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021-08-29 20:11:50.920498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021-08-29 20:11:50.920585: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021-08-29 20:11:50.920624: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-triton-cluster-ray-head-rjkjd): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021-08-29 20:11:50.920879: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021/08/29 20:11:51 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021-08-29 20:11:51.609930: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021-08-29 20:11:52.264623: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021-08-29 20:11:52.264713: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021-08-29 20:11:52.264756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ray-triton-cluster-ray-worker-k9h7b): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021-08-29 20:11:52.265033: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021/08/29 20:11:52 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n",
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021-08-29 20:11:52.975256: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result for MemNNModel_566a9_00000:\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   date: 2021-08-29_20-11-53\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   experiment_id: 9d6dc54f9d8140a9aec5ba426a1f55ba\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   hostname: ray-triton-cluster-ray-head-rjkjd\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   mean_accuracy: 0.171875\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   node_ip: 10.0.23.3\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   pid: 1397\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_since_restore: 2.742231607437134\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_this_iter_s: 2.742231607437134\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_total_s: 2.742231607437134\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timestamp: 1630267913\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   trial_id: 566a9_00000\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Memory usage on this node: 3.4/15.6 GiB\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Resources requested: 2.0/8 CPUs, 0/0 GPUs, 0.0/5.6 GiB heap, 0.0/2.55 GiB objects (0.0/1.0 example-resource-b, 0.0/1.0 example-resource-a)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Current best trial: 566a9_00000 with mean_accuracy=0.171875 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'mixin_example', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-4ext18h5:5432/prisma'}}\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result logdir: /root/ray_results/pbt_babi_memnn\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Number of trials: 2/2 (2 RUNNING)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+----------+----------------+----------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | Trial name             | status   | loc            |      acc |   iter |   total time (s) |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m |------------------------+----------+----------------+----------+--------+------------------|\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00000 | RUNNING  | 10.0.23.3:1397 | 0.171875 |      1 |          2.74223 |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00001 | RUNNING  |                |          |        |                  |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+----------+----------------+----------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1397)\u001b[0m 2021/08/29 20:11:53 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result for MemNNModel_566a9_00000:\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   date: 2021-08-29_20-11-54\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   experiment_id: 9d6dc54f9d8140a9aec5ba426a1f55ba\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   hostname: ray-triton-cluster-ray-head-rjkjd\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   iterations_since_restore: 2\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   mean_accuracy: 0.21875\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   node_ip: 10.0.23.3\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   pid: 1397\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_since_restore: 2.9185242652893066\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_this_iter_s: 0.17629265785217285\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_total_s: 2.9185242652893066\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timestamp: 1630267914\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   training_iteration: 2\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   trial_id: 566a9_00000\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result for MemNNModel_566a9_00001:\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   date: 2021-08-29_20-11-55\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   experiment_id: 3bfbf215518c4959be3d8bb911e59ff5\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   hostname: ray-triton-cluster-ray-worker-k9h7b\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   mean_accuracy: 0.125\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   node_ip: 10.0.23.4\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   pid: 821\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_since_restore: 2.791757106781006\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_this_iter_s: 2.791757106781006\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_total_s: 2.791757106781006\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timestamp: 1630267915\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   trial_id: 566a9_00001\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=821, ip=10.0.23.4)\u001b[0m 2021/08/29 20:11:55 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result for MemNNModel_566a9_00001:\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   date: 2021-08-29_20-11-55\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   experiment_id: 3bfbf215518c4959be3d8bb911e59ff5\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   hostname: ray-triton-cluster-ray-worker-k9h7b\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   iterations_since_restore: 2\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   mean_accuracy: 0.3125\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   node_ip: 10.0.23.4\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   pid: 821\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_since_restore: 2.954185724258423\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_this_iter_s: 0.162428617477417\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   time_total_s: 2.954185724258423\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timestamp: 1630267915\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   training_iteration: 2\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   trial_id: 566a9_00001\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Memory usage on this node: 2.9/15.6 GiB\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m PopulationBasedTraining: 0 checkpoints, 0 perturbs\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.6 GiB heap, 0.0/2.55 GiB objects (0.0/1.0 example-resource-b, 0.0/1.0 example-resource-a)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Current best trial: 566a9_00001 with mean_accuracy=0.3125 and parameters={'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'mixin_example', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-4ext18h5:5432/prisma'}}\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Result logdir: /root/ray_results/pbt_babi_memnn\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m Number of trials: 2/2 (2 TERMINATED)\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+------------+-------+---------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | Trial name             | status     | loc   |     acc |   iter |   total time (s) |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m |------------------------+------------+-------+---------+--------+------------------|\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00000 | TERMINATED |       | 0.21875 |      2 |          2.91852 |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m | MemNNModel_566a9_00001 | TERMINATED |       | 0.3125  |      2 |          2.95419 |\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m +------------------------+------------+-------+---------+--------+------------------+\n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1391)\u001b[0m 2021-08-29 20:11:56,900\tINFO tune.py:550 -- Total run time: 9.81 seconds (8.48 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "        MemNNModel,\n",
    "        name=\"pbt_babi_memnn\",\n",
    "        scheduler=pbt,\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        stop={\"training_iteration\": 2},\n",
    "        num_samples=2,\n",
    "        config={\n",
    "            \"finish_fast\": True,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 1,\n",
    "            \"dropout\": 0.3,\n",
    "            \"lr\": 0.01,\n",
    "            \"rho\": 0.9,\n",
    "            \"mlflow\": {\n",
    "                \"experiment_name\": \"mixin_example\",\n",
    "                \"tracking_uri\": mlflow.get_tracking_uri()\n",
    "            }\n",
    "        },\n",
    "        sync_config=tune.SyncConfig(\n",
    "        sync_to_driver=False,\n",
    "#         upload_dir=\"gs://pipeline_data/ray_data\"\n",
    "        upload_dir=\"s3://d2v-tmp/demo/ray\")\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e9265cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'finish_fast': True, 'batch_size': 32, 'epochs': 1, 'dropout': 0.3, 'lr': 0.01, 'rho': 0.9, 'mlflow': {'experiment_name': 'mixin_example', 'tracking_uri': 'postgresql://postgres:postgres@postgresql.postgres-4ext18h5:5432/prisma'}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", results.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4e0a0",
   "metadata": {},
   "source": [
    "### start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26f9ad0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/ray_results/pbt_babi_memnn/MemNNModel_566a9_00001_1_2021-08-29_20-11-47'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = results.get_best_logdir(\"mean_accuracy\", mode=\"max\")\n",
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef0c52",
   "metadata": {},
   "source": [
    "## setup tensorboard \n",
    "- go to ray head node and run the follwoing command \n",
    "$tensorboard --logdir $logdir\n",
    "- port forward 6006 from head node to localhost\n",
    "- go to localhost:6006 on your browser to see tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb99935",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
