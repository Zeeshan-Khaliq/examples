{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc7df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install raydp==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9583d37e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: kubernetes==18.20 in /opt/conda/lib/python3.8/site-packages (18.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /root/.local/lib/python3.8/site-packages (from kubernetes==18.20) (5.4.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /root/.local/lib/python3.8/site-packages (from kubernetes==18.20) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes==18.20) (1.3.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.8/site-packages (from kubernetes==18.20) (1.26.6)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from kubernetes==18.20) (2.0.2)\n",
      "Requirement already satisfied: requests in /root/.local/lib/python3.8/site-packages (from kubernetes==18.20) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /root/.local/lib/python3.8/site-packages (from kubernetes==18.20) (56.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.8/site-packages (from kubernetes==18.20) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /root/.local/lib/python3.8/site-packages (from kubernetes==18.20) (2.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes==18.20) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes==18.20) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes==18.20) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes==18.20) (4.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes==18.20) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /root/.local/lib/python3.8/site-packages (from requests->kubernetes==18.20) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /root/.local/lib/python3.8/site-packages (from requests->kubernetes==18.20) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib->kubernetes==18.20) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kubernetes==18.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9da311c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray version 1.2.0\n",
      "pandas version 1.1.4\n",
      "raydp version 0.1.1\n",
      "pyspark version 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "import raydp\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "print(f'ray version {ray.__version__}')\n",
    "print(f'pandas version {pd.__version__}')\n",
    "print(f'raydp version {raydp.__version__}')\n",
    "print(f'pyspark version {pyspark.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72127187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 11.0.11 2021-04-20\n",
      "OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b50ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=\"AKIA3YSPHYOF27SKU54H\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=\"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\"\n",
    "os.environ['ISTIO_GATEWAY']=\"istio-m1gxyjhm/ingress-gateway-ahcmlvlf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16627d",
   "metadata": {},
   "source": [
    "### start ray cluster, since we are on the head node, use default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec6f4c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### init from local as we are on the ray headnode\n",
    "# ray.init(address=\"ray://10.1.169.11:8000\")\n",
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed456f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d75124e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-05 20:24:16,939\tINFO worker.py:664 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Hyperplane: selecting worker node pool\n",
      "Waiting for worker ray-worker-a45edba0-5819-4ae3-9da1-4cc4eb9b498a...\n",
      "Waiting for worker ray-worker-2b12e267-54ff-413d-876a-13902e99ee93...\n"
     ]
    }
   ],
   "source": [
    "from ray_common import initialize_ray_cluster, stop_ray_cluster, find_ray_workers\n",
    "num_workers = 2\n",
    "cpu_core_per_worker=4\n",
    "ram_gb_per_worker=4\n",
    "\n",
    "\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_core_per_worker, ram_gb_per_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412d0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba36c194",
   "metadata": {},
   "source": [
    "### change the logging level of spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5678935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/05 20:31:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f38f6",
   "metadata": {},
   "source": [
    "### start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf8d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/ray/jars/ray_dist.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-05 20:31:41 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = raydp.init_spark('example', num_executors=2, executor_cores=4, executor_memory='2G')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3e975",
   "metadata": {},
   "source": [
    "### read tsv data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d132049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "# hadoopConf.set(\"fs.s3a.access.key\", os.environ.get('AWS_ACCESS_KEY_ID'))\n",
    "# hadoopConf.set(\"fs.s3a.secret.key\", os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "# hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "# hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c89ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.csv(path='s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews.tsv', sep ='\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6e1e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|     id|sentiment|              review|\n",
      "+-------+---------+--------------------+\n",
      "| 5814_8|        1|With all this stu...|\n",
      "| 2381_9|        1|\"The Classic War ...|\n",
      "| 7759_3|        0|The film starts w...|\n",
      "| 3630_4|        0|It must be assume...|\n",
      "| 9495_8|        1|Superbly trashy a...|\n",
      "| 8196_8|        1|I dont know why p...|\n",
      "| 7166_2|        0|This movie could ...|\n",
      "|10633_1|        0|I watched this vi...|\n",
      "|  319_1|        0|A friend of mine ...|\n",
      "|8713_10|        1|<br /><br />This ...|\n",
      "| 2486_3|        0|What happens when...|\n",
      "|6811_10|        1|Although I genera...|\n",
      "|11744_9|        1|\"Mr. Harvey Light...|\n",
      "| 7369_1|        0|I had a feeling t...|\n",
      "|12081_1|        0|note to George Li...|\n",
      "| 3561_4|        0|Stephen King adap...|\n",
      "| 4489_1|        0|`The Matrix' was ...|\n",
      "| 3951_2|        0|Ulli Lommel's 198...|\n",
      "|3304_10|        1|This movie is one...|\n",
      "|9352_10|        1|Most people, espe...|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b217444",
   "metadata": {},
   "source": [
    "### do some cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5082c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dropna\n",
    "ds = ds.dropna()\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0d299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|    id|sentiment|              review|        review_clean|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|5814_8|        1|With all this stu...|With all this stu...|\n",
      "|2381_9|        1|\"The Classic War ...|\"The Classic War ...|\n",
      "|7759_3|        0|The film starts w...|The film starts w...|\n",
      "|3630_4|        0|It must be assume...|It must be assume...|\n",
      "|9495_8|        1|Superbly trashy a...|Superbly trashy a...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## remove html tags\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "ds = ds.withColumn(\"review_clean\",regexp_replace(col('review'), '<[^>]+>', ''))\n",
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d20f5",
   "metadata": {},
   "source": [
    "### save cleaned data to parquet on s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e355b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds.write.parquet(\"s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4cbaa",
   "metadata": {},
   "source": [
    "### read back parquet data with pandas to do downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9080eedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id sentiment                                             review  \\\n",
       "0  5814_8         1  With all this stuff going down at the moment w...   \n",
       "1  2381_9         1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  With all this stuff going down at the moment w...  \n",
       "1  \"The Classic War of the Worlds\" by Timothy Hin...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"s3://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01049569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting ray-worker-a45edba0-5819-4ae3-9da1-4cc4eb9b498a\n",
      "Deleting ray-worker-2b12e267-54ff-413d-876a-13902e99ee93\n"
     ]
    }
   ],
   "source": [
    "stop_ray_cluster(ray_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f6cb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray-worker-3de1410a-96f9-4dcf-880f-3eea1cb607e1\tRunning\t10.0.204.5\n",
      "ray-worker-fcfa1786-ebe2-4d33-a547-1c0244c2fcbe\tRunning\t10.0.204.6\n"
     ]
    }
   ],
   "source": [
    "#Use this in case you forgot your workers\n",
    "w = find_ray_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7e4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
