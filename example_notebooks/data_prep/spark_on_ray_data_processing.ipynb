{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbb78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install raydp==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a3bb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install kubernetes==18.20 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44af21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray version 1.2.0\n",
      "pandas version 1.1.4\n",
      "raydp version 0.1.1\n",
      "pyspark version 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "import raydp\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "print(f'ray version {ray.__version__}')\n",
    "print(f'pandas version {pd.__version__}')\n",
    "print(f'raydp version {raydp.__version__}')\n",
    "print(f'pyspark version {pyspark.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ab303c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk 11.0.11 2021-04-20\n",
      "OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f81d7e",
   "metadata": {},
   "source": [
    "### start ray cluster, since we are on the head node, use default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f8652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‰ Hyperplane: selecting worker node pool\n",
      "best pool spec {'pool_env_var': 'DASK_POOL_16_16', 'allocatable_cores': 15.0, 'allocatable_ram': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 05:39:50,251\tINFO services.py:1270 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-12-14 05:39:50,256\tWARNING services.py:1748 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=8.28gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker ray-worker-a61f6f56-d770-4dee-a9e0-e4b645a21368...\n",
      "Waiting for worker ray-worker-a598d2c0-f6fb-4962-964f-21cfc2817443...\n"
     ]
    }
   ],
   "source": [
    "from hyperplane.ray_common import initialize_ray_cluster, stop_ray_cluster, find_ray_workers\n",
    "num_workers = 2\n",
    "cpu_core_per_worker = 15\n",
    "ram_gb_per_worker = 12 #110 GB allocatible for 16_128 nodes, 12 for 16_16 nodes, 27 for 32_32 nodes\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_core_per_worker, ram_gb_per_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583546f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7684096",
   "metadata": {},
   "source": [
    "### change the logging level of spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8479c9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/05 20:31:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "log4j = sc._jvm.org.apache.log4j\n",
    "log4j.LogManager.getRootLogger().setLevel(log4j.Level.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595f307",
   "metadata": {},
   "source": [
    "### start spark session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156ec31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/ray/jars/ray_dist.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-05 20:31:41 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = raydp.init_spark('example', num_executors=2, executor_cores=4, executor_memory='2G')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffcc91",
   "metadata": {},
   "source": [
    "### read tsv data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd13cd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "# hadoopConf.set(\"fs.s3a.access.key\", os.environ.get('AWS_ACCESS_KEY_ID'))\n",
    "# hadoopConf.set(\"fs.s3a.secret.key\", os.environ.get('AWS_SECRET_ACCESS_KEY'))\n",
    "# hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "# hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9613c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = spark.read.csv(path='s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews.tsv', sep ='\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f8ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+\n",
      "|     id|sentiment|              review|\n",
      "+-------+---------+--------------------+\n",
      "| 5814_8|        1|With all this stu...|\n",
      "| 2381_9|        1|\"The Classic War ...|\n",
      "| 7759_3|        0|The film starts w...|\n",
      "| 3630_4|        0|It must be assume...|\n",
      "| 9495_8|        1|Superbly trashy a...|\n",
      "| 8196_8|        1|I dont know why p...|\n",
      "| 7166_2|        0|This movie could ...|\n",
      "|10633_1|        0|I watched this vi...|\n",
      "|  319_1|        0|A friend of mine ...|\n",
      "|8713_10|        1|<br /><br />This ...|\n",
      "| 2486_3|        0|What happens when...|\n",
      "|6811_10|        1|Although I genera...|\n",
      "|11744_9|        1|\"Mr. Harvey Light...|\n",
      "| 7369_1|        0|I had a feeling t...|\n",
      "|12081_1|        0|note to George Li...|\n",
      "| 3561_4|        0|Stephen King adap...|\n",
      "| 4489_1|        0|`The Matrix' was ...|\n",
      "| 3951_2|        0|Ulli Lommel's 198...|\n",
      "|3304_10|        1|This movie is one...|\n",
      "|9352_10|        1|Most people, espe...|\n",
      "+-------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985c80d",
   "metadata": {},
   "source": [
    "### do some cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ebd4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dropna\n",
    "ds = ds.dropna()\n",
    "ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec55f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+\n",
      "|    id|sentiment|              review|        review_clean|\n",
      "+------+---------+--------------------+--------------------+\n",
      "|5814_8|        1|With all this stu...|With all this stu...|\n",
      "|2381_9|        1|\"The Classic War ...|\"The Classic War ...|\n",
      "|7759_3|        0|The film starts w...|The film starts w...|\n",
      "|3630_4|        0|It must be assume...|It must be assume...|\n",
      "|9495_8|        1|Superbly trashy a...|Superbly trashy a...|\n",
      "+------+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## remove html tags\n",
    "from pyspark.sql.functions import col, udf,regexp_replace,isnull\n",
    "ds = ds.withColumn(\"review_clean\",regexp_replace(col('review'), '<[^>]+>', ''))\n",
    "ds.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a80a5",
   "metadata": {},
   "source": [
    "### save cleaned data to parquet on s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bcb9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds.write.parquet(\"s3a://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf92a5",
   "metadata": {},
   "source": [
    "### read back parquet data with pandas to do downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8a91e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id sentiment                                             review  \\\n",
       "0  5814_8         1  With all this stuff going down at the moment w...   \n",
       "1  2381_9         1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "\n",
       "                                        review_clean  \n",
       "0  With all this stuff going down at the moment w...  \n",
       "1  \"The Classic War of the Worlds\" by Timothy Hin...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"s3://d2v-tmp/demo/bach_inference/data/imdb_reviews_clean.parquet\")\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c366c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting ray-worker-a45edba0-5819-4ae3-9da1-4cc4eb9b498a\n",
      "Deleting ray-worker-2b12e267-54ff-413d-876a-13902e99ee93\n"
     ]
    }
   ],
   "source": [
    "stop_ray_cluster(ray_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56551a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray-worker-3de1410a-96f9-4dcf-880f-3eea1cb607e1\tRunning\t10.0.204.5\n",
      "ray-worker-fcfa1786-ebe2-4d33-a547-1c0244c2fcbe\tRunning\t10.0.204.6\n"
     ]
    }
   ],
   "source": [
    "#Use this in case you forgot your workers\n",
    "w = find_ray_workers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd423a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
