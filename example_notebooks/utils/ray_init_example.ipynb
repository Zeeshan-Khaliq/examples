{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7cda0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install 'ray[default]'\n",
    "!pip install kubernetes==18.20 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c300942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-03 17:29:42,058\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "2021-09-03 17:29:42,065\tWARNING services.py:1619 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=Xgb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 2gb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for worker ray-worker-6336d121-481a-478f-8a42-b61aaf73fb8a...\n"
     ]
    }
   ],
   "source": [
    "from ray_common import initialize_ray_cluster, stop_ray_cluster\n",
    "num_workers = 2\n",
    "cpu_per_worker=\"4000m\"\n",
    "ram_per_worker=\"4.0Gi\"\n",
    "\n",
    "ray_cluster = initialize_ray_cluster(num_workers, cpu_per_worker, ram_per_worker)\n",
    "ray_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa766f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kubernetes import client, config, utils\n",
    "# import os\n",
    "# api_instance = client.CoreV1Api()\n",
    "# api_response = api_instance.read_namespaced_pod(\n",
    "#     name=\"ray-worker-45763ed8-a012-4a9b-b67a-59df1b87b866\", \n",
    "#     namespace = os.environ.get('JHUB_NAMESPACE')\n",
    "# )\n",
    "# while True:\n",
    "#     if all([api_response.status.phase==\"Running\", api_response.status.phase==\"Running\"]):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e218cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting ray-worker-8cd71a10-3ce7-4baa-81fc-e5bb11ef7c11\n",
      "Deleting ray-worker-45763ed8-a012-4a9b-b67a-59df1b87b866\n"
     ]
    }
   ],
   "source": [
    "stop_ray_cluster(ray_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a916d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
