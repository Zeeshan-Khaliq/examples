{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5229b8f0",
   "metadata": {},
   "source": [
    "## Convert tensorflow model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "133df233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    " \n",
    "# !{sys.executable} -m pip install --quiet --upgrade tensorflow==2.3.0\n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime==1.4.0\n",
    "!{sys.executable} -m pip install --quiet --upgrade onnxruntime-tools==1.4.0\n",
    "# !{sys.executable} -m pip install --quiet --upgrade keras2onnx==1.7.0\n",
    "# !{sys.executable} -m pip install --quiet transformers==3.0.2\n",
    "# !{sys.executable} -m pip install --quiet onnxconverter_common\n",
    "# !{sys.executable} -m pip install --quiet wget pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d52ae78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "4.4.2\n",
      "usage: Hugging Face ONNX Exporter tool [-h] -m MODEL\n",
      "                                       [--feature {causal-lm,causal-lm-with-past,default,default-with-past,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,sequence-classification-with-past}]\n",
      "                                       [--opset OPSET] [--atol ATOL]\n",
      "                                       output\n",
      "\n",
      "positional arguments:\n",
      "  output                Path indicating where to store generated ONNX model.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model's name of path on disk to load.\n",
      "  --feature {causal-lm,causal-lm-with-past,default,default-with-past,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,sequence-classification-with-past}\n",
      "                        Export the model with some additional feature.\n",
      "  --opset OPSET         ONNX opset version to export the model with (default\n",
      "                        12).\n",
      "  --atol ATOL           Absolute difference tolerence when validating the\n",
      "                        model.\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --quiet --upgrade transformers\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "!python -m transformers.onnx --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7810e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx; print(tf2onnx.__version__)\n",
    "# import keras2onnx; print(keras2onnx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961f596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable_overwrite = False\n",
    "# total_runs = 100\n",
    "# max_sequence_length = 512\n",
    "\n",
    "# import os\n",
    "# cache_dir = './cache_models'\n",
    "# output_dir = './onnx_models'\n",
    "# for directory in [cache_dir, output_dir]:\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "        \n",
    "# import tensorflow as tf\n",
    "# tf.config.set_visible_devices([], 'GPU') # Disable GPU for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68aa60fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import (TFBertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "# #model_name_or_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "# model_name_or_path = \"bert-base-cased\"\n",
    "# is_fine_tuned = (model_name_or_path == 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name_or_path, do_lower_case=True, cache_dir=cache_dir)\n",
    "# model = TFBertForQuestionAnswering.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
    "# # Needed this to export onnx model with multiple inputs with TF 2.2\n",
    "# model._saved_model_inputs_spec = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38e6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "\n",
    "# question, text = \"What is Hyperplane?\", \"Hyperplane  is an end-to-end platform designed to take AI teams from ideation to production at breakthrough speeds. We built Hyperplane because we needed a powerful platform for our scientists to design, develop, deploy and maintain their own work in production.\"\n",
    "# # Pad to max length is needed. Otherwise, position embedding might be truncated by constant folding.\n",
    "# inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors='tf',\n",
    "#                                max_length=max_sequence_length, pad_to_max_length=True, truncation=True)\n",
    "# start_scores, end_scores = model(inputs)\n",
    "\n",
    "# num_tokens = len(inputs[\"input_ids\"][0])\n",
    "# if is_fine_tuned:\n",
    "#     all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "#     print(\"The answer is:\", ' '.join(all_tokens[numpy.argmax(start_scores) : numpy.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f09467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Inference time for sequence length 512 = 881.58 ms\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# for _ in range(total_runs):\n",
    "#     start_scores, end_scores = model(inputs)\n",
    "# end = time.time()\n",
    "# print(\"Tensorflow Inference time for sequence length {} = {} ms\".format(num_tokens, format((end - start) * 1000 / total_runs, '.2f')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488dbd4",
   "metadata": {},
   "source": [
    "## convert to onnx through transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cebbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-4_H-512_A-8.zip\n",
    "# !unzip *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b1aaa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Building PyTorch model from configuration: BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /root/ds-29faf59/starter_notebooks/triton/torch_model/bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 512]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 512]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 512]\n",
      "2021-08-27 22:11:17.066930: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 62509056 exceeds 10% of free system memory.\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [512, 512]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [2048]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [512, 2048]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [512]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [2048, 512]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [512]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [512, 512]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [512]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [512]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [512]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [512, 512]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 512]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ./torch_model/pytorch_model.bin\n",
      "CPU times: user 144 ms, sys: 50.5 ms, total: 194 ms\n",
      "Wall time: 7.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# !export BERT_BASE_DIR=\"/root/ds-29faf59/starter_notebooks/trito/torch_model\"\n",
    "\n",
    "!transformers-cli convert --model_type bert \\\n",
    "  --tf_checkpoint ./torch_model/bert_model.ckpt \\\n",
    "  --config ./torch_model/bert_config.json \\\n",
    "  --pytorch_dump_output ./torch_model/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13461890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matrixprofile 1.1.10 requires protobuf==3.11.2, but you have protobuf 3.17.3 which is incompatible.\n",
      "gql 3.0.0a6 requires graphql-core<3.2,>=3.1.5, but you have graphql-core 2.3.2 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers --quiet\n",
    "!pip install -U tensorflow --quiet\n",
    "!pip install -U torch --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf2b1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow 2.6.0\n",
      "torch 1.7.1+cpu\n",
      "transformers 4.4.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f'tensorflow {tf.__version__}')\n",
    "print(f'torch {torch.__version__}')\n",
    "print(f'transformers {transformers.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71dd8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8490ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "     'Great customer service! The food was delicious! Definitely a come again.',\n",
    "     'The VEGAN options are super fire!!! And the plates come in big portions. Very pleased with this spot, I\\'ll definitely be ordering again',\n",
    "     'Come on, this place is family owned and operated, they are super friendly, the tacos are bomb.',\n",
    "     'This is such a great restaurant. Multiple times during days that we don\\'t want to cook, we\\'ve done takeout here and it\\'s been amazing. It\\'s fast and delicious.',\n",
    "     'Staff is really nice. Food is way better than average. Good cost benefit.',\n",
    "     'pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top.',\n",
    "     'At such a *fine* institution, I find the lack of knowledge and respect for the art appalling',\n",
    "     'If I could give one star I would...I walked out before my food arrived the customer service was horrible!',\n",
    "     'Wow the slowest drive thru I\\'ve ever been at WOWWWW. Horrible I won\\'t be coming back here ever again',\n",
    "     'Service: 1 out of 5 stars. They will mess up your order, not have it ready after 30 mins calling them before. Worst ran family business Ive ever seen.'\n",
    "]\n",
    "\n",
    "y = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de86a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = \"/root/ds-29faf59/wattpad_starters/train/imdb_bert\"\n",
    "\n",
    "# # MODEL_PATH = 'bert-base-uncased'\n",
    "# MAX_LEN = 256\n",
    "\n",
    "# review = x[0]\n",
    "# tkzr = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "# inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "# print(f'review: \\'{review}\\'')\n",
    "# print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "# print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29c5108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(inputs[\"attention_mask\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b628b",
   "metadata": {},
   "source": [
    "### may need to add  model_type key in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43e9549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file /root/ds-29faf59/wattpad_starters/train/imdb_bert/config.json not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 524, in get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\", line 1419, in cached_path\n",
      "    raise EnvironmentError(f\"file {url_or_filename} not found\")\n",
      "OSError: file /root/ds-29faf59/wattpad_starters/train/imdb_bert/config.json not found\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 71, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/onnx/__main__.py\", line 50, in main\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\", line 434, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 515, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 548, in get_config_dict\n",
      "    raise EnvironmentError(msg)\n",
      "OSError: Can't load config for '/root/ds-29faf59/wattpad_starters/train/imdb_bert'. Make sure that:\n",
      "\n",
      "- '/root/ds-29faf59/wattpad_starters/train/imdb_bert' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or '/root/ds-29faf59/wattpad_starters/train/imdb_bert' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx --model=\"/root/ds-29faf59/wattpad_starters/train/imdb_bert\" onnx_models/bert-finetuned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2bc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
