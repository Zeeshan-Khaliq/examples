{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build a PDF QA bot with open source models. We show how to use commercially available opensource models to query your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.0.189 in ./.venv/lib/python3.9/site-packages (0.0.189)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (0.5.14)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (2.9.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (1.26.3)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (1.2.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain==0.0.189) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.189) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.9/site-packages (from pydantic<2,>=1->langchain==0.0.189) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.189) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.189) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.189) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain==0.0.189) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.189) (3.0.3)\n",
      "Requirement already satisfied: packaging>=17.0 in ./.venv/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (23.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.189) (1.0.0)\n",
      "Requirement already satisfied: chromadb==0.3.25 in ./.venv/lib/python3.9/site-packages (0.3.25)\n",
      "Requirement already satisfied: pandas>=1.3 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (2.31.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (1.10.14)\n",
      "Requirement already satisfied: hnswlib>=0.7 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (0.8.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (0.7.0)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (0.9.2)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (0.109.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.27.0.post1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (1.26.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (3.3.4)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (1.16.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (4.9.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.9/site-packages (from chromadb==0.3.25) (7.7.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (2023.11.17)\n",
      "Requirement already satisfied: urllib3>=1.26 in ./.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (2.2.0)\n",
      "Requirement already satisfied: pytz in ./.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (2023.4)\n",
      "Requirement already satisfied: zstandard in ./.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (0.22.0)\n",
      "Requirement already satisfied: lz4 in ./.venv/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.25) (4.3.3)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in ./.venv/lib/python3.9/site-packages (from fastapi>=0.85.1->chromadb==0.3.25) (0.35.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (23.2)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (4.25.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.25) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb==0.3.25) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas>=1.3->chromadb==0.3.25) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb==0.3.25) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb==0.3.25) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests>=2.28->chromadb==0.3.25) (3.6)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.9/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.25) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in ./.venv/lib/python3.9/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (1.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.25) (12.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./.venv/lib/python3.9/site-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.85.1->chromadb==0.3.25) (4.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.25) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.25) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.85.1->chromadb==0.3.25) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.85.1->chromadb==0.3.25) (1.2.0)\n",
      "Requirement already satisfied: pdfplumber==0.9.0 in ./.venv/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in ./.venv/lib/python3.9/site-packages (from pdfplumber==0.9.0) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./.venv/lib/python3.9/site-packages (from pdfplumber==0.9.0) (10.2.0)\n",
      "Requirement already satisfied: Wand>=0.6.10 in ./.venv/lib/python3.9/site-packages (from pdfplumber==0.9.0) (0.6.13)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./.venv/lib/python3.9/site-packages (from pdfminer.six==20221105->pdfplumber==0.9.0) (3.3.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./.venv/lib/python3.9/site-packages (from pdfminer.six==20221105->pdfplumber==0.9.0) (42.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber==0.9.0) (2.21)\n",
      "Requirement already satisfied: tiktoken==0.4.0 in ./.venv/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.9/site-packages (from tiktoken==0.4.0) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.9/site-packages (from tiktoken==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.11.17)\n",
      "Requirement already satisfied: lxml==4.9.2 in ./.venv/lib/python3.9/site-packages (4.9.2)\n",
      "Requirement already satisfied: torch==2.0.1 in ./.venv/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch==2.0.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch==2.0.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch==2.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch==2.0.1) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch==2.0.1) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: transformers==4.29.2 in ./.venv/lib/python3.9/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers==4.29.2) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.29.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.29.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.29.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers==4.29.2) (2023.11.17)\n",
      "Requirement already satisfied: accelerate==0.19.0 in ./.venv/lib/python3.9/site-packages (0.19.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from accelerate==0.19.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from accelerate==0.19.0) (23.2)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.9/site-packages (from accelerate==0.19.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.9/site-packages (from accelerate==0.19.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.9/site-packages (from accelerate==0.19.0) (2.0.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.19.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.19.0) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->accelerate==0.19.0) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->accelerate==0.19.0) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.6.0->accelerate==0.19.0) (1.3.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in ./.venv/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (4.29.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (2.0.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.4.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (1.12.0)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers==2.2.2) (0.20.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.12.2)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.9/site-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.9/site-packages (from torchvision->sentence-transformers==2.2.2) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: einops==0.6.1 in ./.venv/lib/python3.9/site-packages (0.6.1)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.0.5.tar.gz (2.4 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Collecting distribute\n",
      "  \u001b[31m   \u001b[0m   Downloading distribute-0.7.3.zip (145 kB)\n",
      "  \u001b[31m   \u001b[0m \u001b[?25l     \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/145.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/145.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m \u001b[32m143.4/145.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[?25h  Installing build dependencies: started\n",
      "  \u001b[31m   \u001b[0m   Installing build dependencies: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: started\n",
      "  \u001b[31m   \u001b[0m   Getting requirements to build wheel: finished with status 'done'\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): started\n",
      "  \u001b[31m   \u001b[0m   Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "  \u001b[31m   \u001b[0m   \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help [cmd1 cmd2 ...]\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py --help-commands\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m    or: setup.py cmd --help\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m error: invalid command 'dist_info'\n",
      "  \u001b[31m   \u001b[0m   \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "  \u001b[31m   \u001b[0m \u001b[1;36mhint\u001b[0m: See above for details.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install backend dependencies\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "### Installations\n",
    "!pip install langchain==0.0.189\n",
    "!pip install chromadb==0.3.25\n",
    "!pip install pdfplumber==0.9.0\n",
    "!pip install tiktoken==0.4.0\n",
    "!pip install lxml==4.9.2\n",
    "!pip install torch==2.0.1\n",
    "!pip install transformers==4.29.2\n",
    "!pip install accelerate==0.19.0\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install einops==0.6.1\n",
    "!pip install dotenv\n",
    "# !pip install xformers==0.0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenAI Keys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    from hyperplane.utils import is_jhub\n",
    "    if is_jhub(): ##If using jupyter hub and conda base environment\n",
    "        openaiKeyFile = '/root/.secret/openai_key.json'\n",
    "    else:\n",
    "        ## Storing as a secret on k8s cluster\n",
    "        openaiKeyFile = '/etc/hyperplane/secrets/openai_key.json'\n",
    "    with open(openaiKeyFile) as f:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = json.load(f)['openai_key']\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step0: Loading LLM embedding models and generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Embedding Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [INSTRUCTOR XL](https://huggingface.co/hkunlp/instructor-xl): Instructor xl is an instruction-finetuned text embedding model, that can generate embeddings tailored for any task instuction. The instruction for embedding text snippets is \"Represent the document for retrieval:\" and instruction for embedding user question is  \"Represent the question for retrieving supporting documents:\"\n",
    "    * [SBERT](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): SBERT maps sentences and paragraphs to a vectore using BERT like model. It's a good starter when you're prototyping your application\n",
    "* Hugging faces [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) compares embedding models on  different tasks, Instructor XL ranks very high in it, even better than [OpenAI's ADA](https://platform.openai.com/docs/guides/embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_OPENAI_ADA = \"text-embedding-ada-002\"\n",
    "EMB_INSTRUCTOR_XL = \"hkunlp/instructor-xl\"\n",
    "EMB_SBERT_MPNET_BASE = \"sentence-transformers/all-mpnet-base-v2\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Generation Models\n",
    "* Opensource models used in the codebase are\n",
    "    * [FlanT5 Models](https://huggingface.co/docs/transformers/model_doc/flan-t5): FlanT5 is text2text generator that is finetuned on a number of tasks like summarisation, question answering. It uses the encode-decoder architecture of transformers. The model is Apache 2.0 licensed. It can used commercially.\n",
    "    * [FastChatT5 3b Model](https://huggingface.co/sentence-transformers/all-mpnet-base-v2): It's a FlanT5 based chat model trained by finetuning FlanT5 on user chats from ChatGpt. The model is Apache 2.0 licensed.\n",
    "    * [Falcon7b Model](https://huggingface.co/tiiuae/falcon-7b): Falcon7b is a smaller version of Falcon which is text generator model (decoder only model). Falcon-40B is currently the best open source model on the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). They are trained with high quality training data\n",
    "\n",
    "There are other open-source models (MPT-7B, StableLM, RedPajama) in the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) you can consider for your tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_OPENAI_GPT35 = \"gpt-3.5-turbo\"\n",
    "LLM_FLAN_T5_XXL = \"google/flan-t5-xxl\"\n",
    "LLM_FLAN_T5_XL = \"google/flan-t5-xl\"\n",
    "LLM_FASTCHAT_T5_XL = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "LLM_FLAN_T5_SMALL = \"google/flan-t5-small\"\n",
    "LLM_FLAN_T5_BASE = \"google/flan-t5-base\"\n",
    "LLM_FLAN_T5_LARGE = \"google/flan-t5-large\"\n",
    "LLM_FALCON_SMALL = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Falcon, FlanT5 LARGE,XL,XXL, Fastchat model requires GPU to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and first setup SBERT for embedding model and  flant5-base for generation model. Their inference latency is decent on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/examples/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\":LLM_FLAN_T5_BASE,\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be loaded and used for inference with the help of [hugging face pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) which abstract away alot of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sbert_mpnet():\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "\n",
    "\n",
    "def create_flan_t5_base(load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "\n",
    "def create_falcon_instruct_small(load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "    embedding = create_sbert_mpnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in_8bit = config[\"load_in_8bit\"]\n",
    "if config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Ingesting the data into vector store (ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/admin/Downloads/2. BAO Collective agreement.pdf\"\n",
    "loader = PDFPlumberLoader(pdf_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split documents and create text snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "texts = text_splitter.split_documents(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = config[\"persist_directory\"]\n",
    "vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Retriving Snippets and Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA finds relevant snippets based on question embeddings, then construct a Prompt and query LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm = HuggingFacePipeline(pipeline=llm)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=retriever)\n",
    "# , chain_kwargs = {\"return_intermediate_steps\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a default prompt for flan models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"llm\"] == LLM_FLAN_T5_SMALL or config[\"llm\"] == LLM_FLAN_T5_BASE or config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "    question_t5_template = \"\"\"\n",
    "    context: {context}\n",
    "    question: {question}\n",
    "    answer: \n",
    "    \"\"\"\n",
    "    QUESTION_T5_PROMPT = PromptTemplate(\n",
    "        template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Querying LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1551 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Both `max_new_tokens` (=100) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': \"what's the reason for financial crisis?\",\n",
       " 'result': 'industrial conflict',\n",
       " 'source_documents': [Document(page_content='AGREEMENT BETWEEN BAO AND JUSEK /\\nCIVILEKONOMERNA / SVERIGES INGEN-\\nJÖRER ON PROTECTIVE AND SECURITY\\nMEASURES DURING INDUSTRIAL CON-\\nFLICTS\\nBAO and Jusek/Civilekonomerna/Sveriges Ingenjörer note that the current\\ninternationalisation and globalisation of the financial markets entail new\\nconsiderations for the parties regarding the effects and the consequences\\nof industrial conflict in the banking and financial sector.\\nTherefore, the parties agree to the following:\\n1.\\nIn order to avoid major and long-term damage, the banks/compa-\\nnies must be able to fulfil and settle delivery and payment obliga-\\ntions regarding financial commitments undertaken before an indus-\\ntrial conflict. It must be possible to pursue the work that is required\\nto maintain the risk management levels of the bank/company to\\nsatisfy the requirements of the authorities during a conflict.\\nThe banks/companies must therefore have access to RIX, CLS, EBA,\\nTarget, VPC and the corresponding international clearing organisa-\\ntions for securities settlement, OM with the corresponding interna-\\ntional clearing organisations for derivatives settlement and clearing\\nservices at the major international banks.\\n2.\\nIt must be possible to use the S.W.I.F.T system.\\nIn order to fulfil the commitments stated above, a certain level of IT\\n3.\\nservices must be up and running.\\nIt is considered protective / security work to monitor computer\\nsystems in order to deal with a major crash or to ward off any pos-\\nsible intrusion attempts or virus attacks.\\nThe computer systems are so integrated that it is not viable to shut\\ndown individual systems in a safe and secure way.\\nThe bank and the local organisation of Jusek/Civilekonomerna/Sve-\\nriges Ingenjörer shall decide which employees shall be exempt from\\nthe conflict in order to fulfil measures under 1-3.\\nIf there is a need to exempt certain functions regarding the opera-\\ntions of the bank located abroad, local agreements may be con-\\ncluded but shall be centrally approved by BAO and Jusek/Civi-\\nlekonomerna/Sveriges Ingenjörer.\\n41', metadata={'source': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'file_path': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'page': 41, 'total_pages': 42, 'Author': 'Anna-Mi Björkvik', 'Comments': '\\n  ', 'Company': '\\n  ', 'ContentTypeId': '0x010100C1BD4575293719449D4185BC4A5AAFDD', 'CreationDate': \"D:20210225155035+01'00'\", 'Creator': 'Acrobat PDFMaker 21 för Word', 'Keywords': '\\n  ', 'ModDate': \"D:20220203160003+01'00'\", 'Producer': 'Adobe PDF Library 21.1.170', 'SourceModified': 'D:20210225144928', 'Subject': '\\n  ', 'Title': '\\n  '}),\n",
       "  Document(page_content='Collective Agreement\\nbetween\\nThe Employers of the Financial Sector\\n(BAO)\\nand\\nJusek/Civilekonomerna/Sveriges Ingenjörer\\nfor graduate professionals\\nEffective from 1 January 2015\\nThis translation is made for convenience only and in case of any discrepancy, the Swe-\\ndish-language version shall control.\\nThis translated version of the collective agreement is published bilaterally by BAO and\\nJusek/Civilekonomerna/Sveriges Ingenjörer.\\n1', metadata={'source': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'file_path': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'page': 1, 'total_pages': 42, 'Author': 'Anna-Mi Björkvik', 'Comments': '\\n  ', 'Company': '\\n  ', 'ContentTypeId': '0x010100C1BD4575293719449D4185BC4A5AAFDD', 'CreationDate': \"D:20210225155035+01'00'\", 'Creator': 'Acrobat PDFMaker 21 för Word', 'Keywords': '\\n  ', 'ModDate': \"D:20220203160003+01'00'\", 'Producer': 'Adobe PDF Library 21.1.170', 'SourceModified': 'D:20210225144928', 'Subject': '\\n  ', 'Title': '\\n  '}),\n",
       "  Document(page_content='§ 1. SCOPE OF THE AGREEMENT\\n1. Scope of the Agreement\\nThis agreement applies to employees who are members of Jusek (The Swe-\\ndish Union of University Graduates of Law, Business Administration and\\nEconomics, Computer and Systems Science, Personnel Management, Pro-\\nfessional Communicators and Social Science)/Civilekonomerna (The Swe-\\ndish Association of Graduates in Business Administration and Econom-\\nics)/Sveriges Ingenjörer (The Swedish Association of Graduate Engineers)\\nand also of Saco (The Swedish Confederation of Professional Associations)\\nassociations, which are members of the PTK (the Council for Negotiation\\nand Cooperation) represented at all banks which are members of BAO with\\nthe following exceptions:\\na) Employees (cleaning and canteen staff etc.) who do not work in the\\nbanking operations and for whom a separate agreement has been\\nconcluded in certain cases.\\n1.\\nb) The following employees\\n2.\\nManaging Director/CEO\\n3.\\nThe secretary to the Managing Director/CEO\\n4.\\nSenior Executives\\n5.\\nHR Manager or equivalent at the head office of the bank\\nCertain employees according to an agreement between the\\nbank and the local organisation of Jusek/Civi-\\nlekonomerna/Sveriges Ingenjörer\\n1.\\nc) The following employees are exempt from labour conflicts\\n2.\\nThe Managing Director/CEO\\n3.\\nThe secretary to the Managing Director/CEO\\n4.\\nA Deputy Managing Director or Acting Managing Director\\nSenior Executives who are part of the management group at\\n5.\\nthe head office or central company management\\nThe HR Manager at the head office, regional bank or corre-\\n6.\\nsponding unit\\nMember of management or equivalent position at a regional\\n7.\\nbank or corresponding unit\\nCertain employees according to an agreement between the\\nbank and Jusek/Civilekonomerna/Sveriges Ingenjörer\\nJusek/Civilekonomerna/Sveriges Ingenjörer are prepared to allow further\\nexNeomtep: tions from industrial action in the case of labour conflicts.\\nJusek/Civilekonomerna/Sveriges Ingenjörer have declared that members with positions\\nsuch as Managing Director, Deputy Managing Director, Administrative Manager, HR Man-\\nager or other central positions with equivalent working tasks may not take on any trade\\nunion assignments or be actively involved in local trade union activities.\\n4', metadata={'source': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'file_path': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'page': 4, 'total_pages': 42, 'Author': 'Anna-Mi Björkvik', 'Comments': '\\n  ', 'Company': '\\n  ', 'ContentTypeId': '0x010100C1BD4575293719449D4185BC4A5AAFDD', 'CreationDate': \"D:20210225155035+01'00'\", 'Creator': 'Acrobat PDFMaker 21 för Word', 'Keywords': '\\n  ', 'ModDate': \"D:20220203160003+01'00'\", 'Producer': 'Adobe PDF Library 21.1.170', 'SourceModified': 'D:20210225144928', 'Subject': '\\n  ', 'Title': '\\n  '}),\n",
       "  Document(page_content='Local negotiations shall be initiated as quickly as possible and at the latest\\nwithin ten banking days from the day they were requested unless the local\\nparties have agreed to a postponement.\\nCentral negotiations\\nThe aim of this agreement is that co-determination is exercised through the\\nlocal parties. Thus, central negotiations between BAO and Jusek/Civi-\\nlekonomerna/Sveriges Ingenjörer should only be initiated for questions\\nthat are broader in nature or of principle importance. Central negotiations\\nshall take place at the joint request of the bank and the local organisation of\\nJusek/Civilekonomerna/Sveriges Ingenjörer or at the request of either BAO\\nor Jusek/ Civilekonomerna/Sveriges Ingenjörer.\\nCentral negotiations shall be requested within ten banking days after the\\nlocal negotiations concluded. Central negotiations shall be initiated as\\nquickly as possible and at the latest within ten banking days from the day\\nthey were requested unless the central parties have agreed to a postpone-\\nment.\\nFiduciary council\\nAny dispute arising out of or in connection with the interpretation of this\\nagreement or of provisions or decisions based on this agreement, a local\\nagreement or the MBL and where no solution has been found during the\\ncentral negotiations, may be referred at the request of either party within\\nten banking days from the day that the central negotiations were concluded\\nto the joint fiduciary council of BAO and Jusek/Civilekonomerna/Sveriges\\nIngenjörer in accordance with Section 9 Rules of procedure for negotia-\\ntions – arbitral tribunal in the Collective Agreement.\\n40', metadata={'source': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'file_path': '/Users/admin/Downloads/2. BAO Collective agreement.pdf', 'page': 40, 'total_pages': 42, 'Author': 'Anna-Mi Björkvik', 'Comments': '\\n  ', 'Company': '\\n  ', 'ContentTypeId': '0x010100C1BD4575293719449D4185BC4A5AAFDD', 'CreationDate': \"D:20210225155035+01'00'\", 'Creator': 'Acrobat PDFMaker 21 för Word', 'Keywords': '\\n  ', 'ModDate': \"D:20220203160003+01'00'\", 'Producer': 'Adobe PDF Library 21.1.170', 'SourceModified': 'D:20210225144928', 'Subject': '\\n  ', 'Title': '\\n  '})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what's the reason for financial crisis?\"\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.return_source_documents = True\n",
    "qa({\"query\":question,})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pack all the functionalities into a class. We have also added Other opensource LLM Models into the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PdfQA:\n",
    "    def __init__(self,config:dict = {}):\n",
    "        self.config = config\n",
    "        self.embedding = None\n",
    "        self.vectordb = None\n",
    "        self.llm = None\n",
    "        self.qa = None\n",
    "        self.retriever = None\n",
    "\n",
    "    # The following class methods are useful to create global GPU model instances\n",
    "    # This way we don't need to reload models in an interactive app,\n",
    "    # and the same model instance can be used across multiple user sessions\n",
    "    @classmethod\n",
    "    def create_instructor_xl(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceInstructEmbeddings(model_name=EMB_INSTRUCTOR_XL, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_sbert_mpnet(cls):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={\"device\": device})\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_xxl(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-xxl with 8-bit quantization for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xxl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=\"google/flan-t5-xl\",\n",
    "            max_new_tokens=200,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_flan_t5_small(cls, load_in_8bit=False):\n",
    "        # Local flan-t5-small for inference\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-small\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_base(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-base\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_flan_t5_large(cls, load_in_8bit=False):\n",
    "        # Wrap it in HF pipeline for use with LangChain\n",
    "        model=\"google/flan-t5-large\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    @classmethod\n",
    "    def create_fastchat_t5_xl(cls, load_in_8bit=False):\n",
    "        return pipeline(\n",
    "            task=\"text2text-generation\",\n",
    "            model = \"lmsys/fastchat-t5-3b-v1.0\",\n",
    "            max_new_tokens=100,\n",
    "            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": load_in_8bit, \"max_length\": 512, \"temperature\": 0.}\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def create_falcon_instruct_small(cls, load_in_8bit=False):\n",
    "        model = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        hf_pipeline = pipeline(\n",
    "                task=\"text-generation\",\n",
    "                model = model,\n",
    "                tokenizer = tokenizer,\n",
    "                trust_remote_code = True,\n",
    "                max_new_tokens=100,\n",
    "                model_kwargs={\n",
    "                    \"device_map\": \"auto\", \n",
    "                    \"load_in_8bit\": load_in_8bit, \n",
    "                    \"max_length\": 512, \n",
    "                    \"temperature\": 0.01,\n",
    "                    \"torch_dtype\":torch.bfloat16,\n",
    "                    }\n",
    "            )\n",
    "        return hf_pipeline\n",
    "    \n",
    "    def init_embeddings(self) -> None:\n",
    "        # OpenAI ada embeddings API\n",
    "        if self.config[\"embedding\"] == EMB_OPENAI_ADA:\n",
    "            self.embedding = OpenAIEmbeddings()\n",
    "        elif self.config[\"embedding\"] == EMB_INSTRUCTOR_XL:\n",
    "            # Local INSTRUCTOR-XL embeddings\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_instructor_xl()\n",
    "        elif self.config[\"embedding\"] == EMB_SBERT_MPNET_BASE:\n",
    "            ## this is for SBERT\n",
    "            if self.embedding is None:\n",
    "                self.embedding = PdfQA.create_sbert_mpnet()\n",
    "        else:\n",
    "            self.embedding = None ## DuckDb uses sbert embeddings\n",
    "            # raise ValueError(\"Invalid config\")\n",
    "\n",
    "    def init_models(self) -> None:\n",
    "        \"\"\" Initialize LLM models based on config \"\"\"\n",
    "        load_in_8bit = self.config.get(\"load_in_8bit\",False)\n",
    "        # OpenAI GPT 3.5 API\n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "            # OpenAI GPT 3.5 API\n",
    "            pass\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_small(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_BASE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_base(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_large(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FLAN_T5_XXL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_flan_t5_xxl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_fastchat_t5_xl(load_in_8bit=load_in_8bit)\n",
    "        elif self.config[\"llm\"] == LLM_FALCON_SMALL:\n",
    "            if self.llm is None:\n",
    "                self.llm = PdfQA.create_falcon_instruct_small(load_in_8bit=load_in_8bit)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid config\")        \n",
    "    def vector_db_pdf(self) -> None:\n",
    "        \"\"\"\n",
    "        creates vector db for the embeddings and persists them or loads a vector db from the persist directory\n",
    "        \"\"\"\n",
    "        pdf_path = self.config.get(\"pdf_path\",None)\n",
    "        persist_directory = self.config.get(\"persist_directory\",None)\n",
    "        if persist_directory and os.path.exists(persist_directory):\n",
    "            ## Load from the persist db\n",
    "            self.vectordb = Chroma(persist_directory=persist_directory, embedding_function=self.embedding)\n",
    "        elif pdf_path and os.path.exists(pdf_path):\n",
    "            ## 1. Extract the documents\n",
    "            loader = PDFPlumberLoader(pdf_path)\n",
    "            documents = loader.load()\n",
    "            ## 2. Split the texts\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "            texts = text_splitter.split_documents(documents)\n",
    "            # text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10, encoding_name=\"cl100k_base\")  # This the encoding for text-embedding-ada-002\n",
    "            text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)  # This the encoding for text-embedding-ada-002\n",
    "            texts = text_splitter.split_documents(texts)\n",
    "\n",
    "            ## 3. Create Embeddings and add to chroma store\n",
    "            ##TODO: Validate if self.embedding is not None\n",
    "            self.vectordb = Chroma.from_documents(documents=texts, embedding=self.embedding, persist_directory=persist_directory)\n",
    "        else:\n",
    "            raise ValueError(\"NO PDF found\")\n",
    "\n",
    "    def retreival_qa_chain(self):\n",
    "        \"\"\"\n",
    "        Creates retrieval qa chain using vectordb as retrivar and LLM to complete the prompt\n",
    "        \"\"\"\n",
    "        ##TODO: Use custom prompt\n",
    "        self.retriever = self.vectordb.as_retriever(search_kwargs={\"k\":3})\n",
    "        \n",
    "        if self.config[\"llm\"] == LLM_OPENAI_GPT35:\n",
    "          # Use ChatGPT API\n",
    "          self.qa = RetrievalQA.from_chain_type(llm=OpenAI(model_name=LLM_OPENAI_GPT35, temperature=0.), chain_type=\"stuff\",\\\n",
    "                                      retriever=self.vectordb.as_retriever(search_kwargs={\"k\":3}))\n",
    "        else:\n",
    "            hf_llm = HuggingFacePipeline(pipeline=self.llm,model_id=self.config[\"llm\"])\n",
    "\n",
    "            self.qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type=\"stuff\",retriever=self.retriever)\n",
    "            if self.config[\"llm\"] == LLM_FLAN_T5_SMALL or self.config[\"llm\"] == LLM_FLAN_T5_BASE or self.config[\"llm\"] == LLM_FLAN_T5_LARGE:\n",
    "                question_t5_template = \"\"\"\n",
    "                context: {context}\n",
    "                question: {question}\n",
    "                answer: \n",
    "                \"\"\"\n",
    "                QUESTION_T5_PROMPT = PromptTemplate(\n",
    "                    template=question_t5_template, input_variables=[\"context\", \"question\"]\n",
    "                )\n",
    "                self.qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT\n",
    "            self.qa.combine_documents_chain.verbose = True\n",
    "            self.qa.return_source_documents = True\n",
    "    def answer_query(self,question:str) ->str:\n",
    "        \"\"\"\n",
    "        Answer the question\n",
    "        \"\"\"\n",
    "\n",
    "        answer_dict = self.qa({\"query\":question,})\n",
    "        print(answer_dict)\n",
    "        answer = answer_dict[\"result\"]\n",
    "        if self.config[\"llm\"] == LLM_FASTCHAT_T5_XL:\n",
    "            answer = self._clean_fastchat_t5_output(answer)\n",
    "        return answer\n",
    "    def _clean_fastchat_t5_output(self, answer: str) -> str:\n",
    "        # Remove <pad> tags, double spaces, trailing newline\n",
    "        answer = re.sub(r\"<pad>\\s+\", \"\", answer)\n",
    "        answer = re.sub(r\"  \", \" \", answer)\n",
    "        answer = re.sub(r\"\\n$\", \"\", answer)\n",
    "        return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PDFQA class is set up, you can initialize and run it as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████████████████████████████████████████████████| 1.52k/1.52k [00:00<00:00, 3.74MB/s]\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▍| 6.63G/6.71G [12:06<00:07, 10.7MB/s]Error while downloading from https://cdn-lfs.huggingface.co/repos/b9/1a/b91a75c3d5600e9c3caa75085c34f04fd6a75b1d45612fe045ce4ac593cf5167/2cd1b91cbf3eb21032792128bf46e2bf21df2a9494542ed479037e3f7544c5cb?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1706913310&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNjkxMzMxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iOS8xYS9iOTFhNzVjM2Q1NjAwZTljM2NhYTc1MDg1YzM0ZjA0ZmQ2YTc1YjFkNDU2MTJmZTA0NWNlNGFjNTkzY2Y1MTY3LzJjZDFiOTFjYmYzZWIyMTAzMjc5MjEyOGJmNDZlMmJmMjFkZjJhOTQ5NDU0MmVkNDc5MDM3ZTNmNzU0NGM1Y2I%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=P9163mPLc9JGndZcPYJFaVOTDwdM8DHdSW7tReLpdFH8JWh5syEnQqLYnLioL82iIhrPA1unXSas89yFNMc1-Fgi6iMCoBaigzCTgwv6mxny5qzMsrmLaxJ3RBL0rlOARMYKnKFGasyWQB9TOEPgm7phRANEY6o5rIiLrPMrUVD9AXx3R6tlEgYpBezAzC2SiDsKZljMeyoDiLh1Be%7E0FnLZsQyUwAK6KzKkhIhkp3wLp0cG2g0Q6UU4tyErFCybCg-Re3O0vl4LqhSmqvS-DYr4NcgPr3ZY-nCmv%7Ezcotc8f51dRHwo1%7EFxDmVd27ASRqYChvhaA6rjeVn42wyc9g__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████████████▎| 6.63G/6.71G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▌| 6.64G/6.71G [00:00<00:06, 10.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▌| 6.65G/6.71G [00:01<00:05, 10.9MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▋| 6.66G/6.71G [00:02<00:04, 10.4MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▋| 6.67G/6.71G [00:03<00:03, 10.8MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████▊| 6.68G/6.71G [00:04<00:02, 10.6MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████▉| 6.69G/6.71G [00:05<00:01, 10.4MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|███████████████████████████████████████████████▉| 6.70G/6.71G [00:06<00:00, 10.6MB/s]\u001b[A\n",
      "pytorch_model.bin: 100%|████████████████████████████████████████████████| 6.71G/6.71G [00:07<00:00, 10.7MB/s]\u001b[A\n",
      "pytorch_model.bin:  99%|███████████████████████████████████████████████▍| 6.63G/6.71G [12:25<00:08, 8.90MB/s]\n",
      "generation_config.json: 100%|███████████████████████████████████████████████| 142/142 [00:00<00:00, 34.2kB/s]\n",
      "tokenizer_config.json: 100%|████████████████████████████████████████████| 2.40k/2.40k [00:00<00:00, 6.33MB/s]\n",
      "spiece.model: 100%|███████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 2.71MB/s]\n",
      "added_tokens.json: 100%|████████████████████████████████████████████████████| 150/150 [00:00<00:00, 62.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 5.78MB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m pdfqa \u001b[38;5;241m=\u001b[39m PdfQA(config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m     11\u001b[0m pdfqa\u001b[38;5;241m.\u001b[39minit_embeddings()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mpdfqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Create Vector DB \u001b[39;00m\n\u001b[1;32m     15\u001b[0m pdfqa\u001b[38;5;241m.\u001b[39mvector_db_pdf()\n",
      "Cell \u001b[0;32mIn[14], line 149\u001b[0m, in \u001b[0;36mPdfQA.init_models\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m LLM_FASTCHAT_T5_XL:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mPdfQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_fastchat_t5_xl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m LLM_FALCON_SMALL:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[14], line 81\u001b[0m, in \u001b[0;36mPdfQA.create_fastchat_t5_xl\u001b[0;34m(cls, load_in_8bit)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_fastchat_t5_xl\u001b[39m(\u001b[38;5;28mcls\u001b[39m, load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2text-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlmsys/fastchat-t5-3b-v1.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_8bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/pipelines/__init__.py:885\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    883\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 885\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:693\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    691\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1812\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1810\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1821\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1975\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1973\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1975\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1977\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1978\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1979\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1980\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:133\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m extra_tokens \u001b[38;5;241m!=\u001b[39m extra_ids:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth extra_ids (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextra_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and additional_special_tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_special_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m provided to T5Tokenizer. In this case the additional_special_tokens must include the extra_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_save_slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1303\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconverted()\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:445\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    441\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentencepiece_model_pb2 \u001b[38;5;28;01mas\u001b[39;00m model_pb2\n\u001b[1;32m    447\u001b[0m m \u001b[38;5;241m=\u001b[39m model_pb2\u001b[38;5;241m.\u001b[39mModelProto()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/transformers/utils/sentencepiece_model_pb2.py:91\u001b[0m\n\u001b[1;32m     25\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m     28\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[1;32m     29\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece_model.proto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m )\n\u001b[1;32m     84\u001b[0m _TRAINERSPEC_MODELTYPE \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[1;32m     85\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m     full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece.TrainerSpec.ModelType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m     file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[1;32m     89\u001b[0m     create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m     90\u001b[0m     values\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m---> 91\u001b[0m         \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValueDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUNIGRAM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m            \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcreate_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_create_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     99\u001b[0m         _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    100\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    101\u001b[0m             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    102\u001b[0m             number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    103\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m             create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m    106\u001b[0m         ),\n\u001b[1;32m    107\u001b[0m         _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    108\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWORD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    110\u001b[0m             number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    111\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m             create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m    114\u001b[0m         ),\n\u001b[1;32m    115\u001b[0m         _descriptor\u001b[38;5;241m.\u001b[39mEnumValueDescriptor(\n\u001b[1;32m    116\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHAR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    117\u001b[0m             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    118\u001b[0m             number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m    119\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m             create_key\u001b[38;5;241m=\u001b[39m_descriptor\u001b[38;5;241m.\u001b[39m_internal_create_key,\n\u001b[1;32m    122\u001b[0m         ),\n\u001b[1;32m    123\u001b[0m     ],\n\u001b[1;32m    124\u001b[0m     containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m     serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m     serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1294\u001b[39m,\n\u001b[1;32m    127\u001b[0m     serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1347\u001b[39m,\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    129\u001b[0m _sym_db\u001b[38;5;241m.\u001b[39mRegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\n\u001b[1;32m    131\u001b[0m _MODELPROTO_SENTENCEPIECE_TYPE \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mEnumDescriptor(\n\u001b[1;32m    132\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece.ModelProto.SentencePiece.Type\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2184\u001b[39m,\n\u001b[1;32m    191\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/examples/lib/python3.11/site-packages/google/protobuf/descriptor.py:789\u001b[0m, in \u001b[0;36mEnumValueDescriptor.__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, index, number,\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m    788\u001b[0m             options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 789\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m   \u001b[38;5;66;03m# There is no way we can build a complete EnumValueDescriptor with the\u001b[39;00m\n\u001b[1;32m    791\u001b[0m   \u001b[38;5;66;03m# given parameters (the name of the Enum is not known, for example).\u001b[39;00m\n\u001b[1;32m    792\u001b[0m   \u001b[38;5;66;03m# Fortunately generated files just pass it to the EnumDescriptor()\u001b[39;00m\n\u001b[1;32m    793\u001b[0m   \u001b[38;5;66;03m# constructor, which will ignore it, so returning None is good enough.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "# Configuration for PDFQA\n",
    "config = {\"persist_directory\":None,\n",
    "          \"load_in_8bit\":False,\n",
    "          \"embedding\" : EMB_SBERT_MPNET_BASE,\n",
    "          \"llm\": LLM_FASTCHAT_T5_XL, #LLM_FLAN_T5_BASE,\n",
    "          \"pdf_path\": \"/Users/admin/Downloads/2. BAO Collective agreement.pdf\"\n",
    "          }\n",
    "\n",
    "# Initialize PDFQA\n",
    "pdfqa = PdfQA(config=config)\n",
    "pdfqa.init_embeddings()\n",
    "pdfqa.init_models()\n",
    "\n",
    "# Create Vector DB \n",
    "pdfqa.vector_db_pdf()\n",
    "\n",
    "# Set up Retrieval QA Chain\n",
    "pdfqa.retreival_qa_chain()\n",
    "\n",
    "# Query the model\n",
    "question = \"what the reason for financial crisis?\"\n",
    "pdfqa.answer_query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many holidays is an employee entitled to?\"\n",
    "pdfqa.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"If an employee is absent for a full calendar year, will the holiday pay be paid?\"\n",
    "pdfqa.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the notice period if employed for more than 2 years?\"\n",
    "pdfqa.answer_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
